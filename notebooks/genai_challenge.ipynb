{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LBfd8mOeLv8S",
        "gwaNKbjcWKOe",
        "esCZgvywbYjT",
        "GfgYubBVQt1O",
        "4Ya9bvXnOqcz",
        "707gSPDqTnfP",
        "QrAL-SzDaxiy",
        "CFXadmWqZ9e9",
        "Vi1rLOek0EkU",
        "4bQ9EqJR3SCM",
        "KSTsu3nN51uO",
        "WrLRR9YeUVSY",
        "iEBYiXNNtFd3",
        "oH5fGMahI_JD",
        "UW0chWCSBYeF",
        "aVzsipwkIriq",
        "31tnbXtIQlu-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "  <img src=\"https://www.iasonltd.com/logo_full.svg\" alt=\"Polimi Logo\" width=\"200\">\n",
        "  <h4></h4>\n",
        "  <h1> GenAI for Data Analysis </h1>\n",
        "</center>"
      ],
      "metadata": {
        "id": "EOa7RnhfKM8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "LBfd8mOeLv8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to the **GenAI for Data Analysis Challenge!** In this notebook, you'll explore how to leverage **Generative AI** techniques for data analysis, focusing on tool and model structures using **LangChain** with a Large Language Model (LLM), **LLaMA**. Your goal will be to analyze a **Yahoo Finance dataset** and build a model capable of generating automatic summaries and providing insightful feedback based on user queries.\n",
        "\n",
        "As part of this challenge, you will learn:\n",
        "\n",
        "- **Prompt engineering**: Learn how to craft effective prompts that guide the LLM to produce accurate and relevant results.\n",
        "- **Binding tools to the LLM**: Discover how to enhance the LLM’s functionality by integrating external tools, making it more versatile and capable of complex data operations.\n",
        "- **Utilizing LangGraph**: A key part of the challenge will involve working with **LangGraph**, a powerful framework that simplifies the process of connecting different tools and models, allowing you to build more dynamic and interactive AI systems.\n",
        "\n",
        "By the end of this notebook, you will have hands-on experience in setting up a robust AI pipeline that transforms raw data into actionable insights and user-friendly summaries, all through the seamless integration of LangChain.\n"
      ],
      "metadata": {
        "id": "AfsdFiioMViz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview of LangChain"
      ],
      "metadata": {
        "id": "Qy9ezjrNUlQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LangChain** is a versatile framework designed to simplify the development of applications that leverage large language models (LLMs). It offers abstractions for creating custom tools, managing prompts and templates, handling conversational memory, and orchestrating complex workflows. LangChain empowers developers to build sophisticated data agents that interact seamlessly with various data sources and provide intelligent, context-aware responses."
      ],
      "metadata": {
        "id": "uLiepjqQUpZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Features:**\n",
        "\n",
        "\n",
        "*   **Prompt Management:** Create and manage dynamic prompts and templates.\n",
        "*   **Tool Integration:** Connect LLMs with external APIs and data sources.\n",
        "*   **Memory Handling:** Maintain conversational context and state across interactions.\n",
        "*   **Chain Building:** Orchestrate multi-step processes and workflows.\n",
        "\n"
      ],
      "metadata": {
        "id": "dkWJjdPZUvCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For comprehensive documentation, visit the [LangChain Documentation](https://python.langchain.com/docs/introduction/#:~:text=LangChain%20is%20a%20framework%20for%20developing)."
      ],
      "metadata": {
        "id": "LOlyze3rVdlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup"
      ],
      "metadata": {
        "id": "gwaNKbjcWKOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before diving into the implementation, ensure that all necessary libraries are installed."
      ],
      "metadata": {
        "id": "t7560z4AWMw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install langchain transformers pandas matplotlib torch langgraph langchain-ollama langchain_experimental duckduckgo-search --quiet"
      ],
      "metadata": {
        "id": "6QyUXnU3WMJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce2c0e49-5578-487d-a261-2bd41f12efbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.5/113.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.1/208.1 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.7/407.7 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note: The `--quiet` flag suppresses installation logs for a cleaner notebook interface.* *italicized text*"
      ],
      "metadata": {
        "id": "rhKTl2IXWUom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "abnERtgkWxBr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all necessary libraries for data manipulation, modeling, and LangChain functionalities."
      ],
      "metadata": {
        "id": "5H-RARkKWzA5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0U_OaMgKFBa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# LangChain imports\n",
        "from langchain import LLMChain, PromptTemplate\n",
        "from langchain.agents import Tool, initialize_agent\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from IPython.display import display, Markdown\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import ToolMessage\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.messages.system import SystemMessage\n",
        "import operator\n",
        "from typing import Annotated, Sequence, TypedDict\n",
        "from langgraph.graph.message import AnyMessage, add_messages\n",
        "from langchain_core.messages import BaseMessage\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from typing import List\n",
        "from langchain_core.prompts import (ChatPromptTemplate,AIMessagePromptTemplate, MessagesPlaceholder)\n",
        "from langchain_core.messages import ToolMessage\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langgraph.prebuilt import ToolInvocation\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from IPython.display import Image, display\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "from langchain_experimental.utilities import PythonREPL"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up the Language Model: Llama"
      ],
      "metadata": {
        "id": "esCZgvywbYjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Ollama?**\n",
        "\n",
        "**Ollama** is an API platform that provides access to various machine learning models, including **LLaMA** and other advanced models. It allows users to easily interact with these models by offering an accessible and scalable API. With Ollama, you can perform tasks such as text generation, summarization, translation, and more using Large Language Models (LLMs).\n",
        "\n",
        "In the context of this notebook, **Ollama** enables you to download and run models like **LLaMA** locally or remotely with minimal setup. This simplifies the process of integrating powerful machine learning models into your project without needing to manage the intricate details of model training, deployment, or scaling.\n",
        "\n",
        "\n",
        "This code first installs `pciutils`, a set of tools used to manage PCI devices on the system. Then, it downloads and installs the Ollama API by executing a shell script fetched from the Ollama website using `curl`."
      ],
      "metadata": {
        "id": "7y6rvUeJl2eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh # download ollama api"
      ],
      "metadata": {
        "id": "ffpZTxHobWQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `ollama()` configures the environment for the Ollama API by setting the `OLLAMA_HOST` to `0.0.0.0:11434` and allowing all origins through the `OLLAMA_ORIGINS` environment variable. It then starts the Ollama server by invoking the `ollama serve` command in a subprocess. A separate thread is initiated using Python's `threading` module to run the `ollama()` function concurrently, enabling the server to operate in the background while the main program continues execution.\n"
      ],
      "metadata": {
        "id": "HwXehdu6mC0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "gGvL6dRowjKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The command `!ollama pull llama3.2:3b` pulls the specific version `3b` of the `Llama 3.2` model using the Ollama API. After the model is downloaded, the `clear_output()` function is used to clear the output of the current cell, keeping the notebook interface clean. Additionally, different versions of the Llama model can be selected by specifying alternative version tags in the command, depending on the desired model size or version.\n"
      ],
      "metadata": {
        "id": "zyv2kZZwmVp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3.2:3b\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "FAgteKakwnAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `ChatOllama` object from LangChain is initialized with the `llama3.2:3b` model. We set the `temperature` to `0` for deterministic responses and `num_predict` to `-1` to allow unlimited token generation. LangChain's `ChatOllama` is used for seamless integration with the Ollama API, simplifying model interaction.\n"
      ],
      "metadata": {
        "id": "_fXxJC-qmtkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOllama(model=\"llama3.2:3b\", temperature = 0, num_predict = -1)"
      ],
      "metadata": {
        "id": "Otl-MWNHw7-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try out the model! You can ask queries to observe how the model responds to various inputs."
      ],
      "metadata": {
        "id": "kEsYtJ4KnQyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(\"Hello, how are you?\")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZYo_GfHzmg6",
        "outputId": "eec1217c-86d8-4dda-b6b0-d42c86ad6ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm just a language model, so I don't have emotions or feelings like humans do. However, I'm functioning properly and ready to assist you with any questions or tasks you may have! How can I help you today?\", additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2024-10-22T08:51:57.911204847Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 2219369354, 'load_duration': 77858664, 'prompt_eval_count': 31, 'prompt_eval_duration': 25897000, 'eval_count': 47, 'eval_duration': 2072947000}, id='run-57620d74-7f4d-4a59-bdcd-eaa6cde3e508-0', usage_metadata={'input_tokens': 31, 'output_tokens': 47, 'total_tokens': 78})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The response includes detailed metadata, such as the number of input, output, and total tokens, along with additional information like model version, timestamps, and evaluation durations. This allows you to track the performance and usage of the model for each interaction.\n",
        "\n",
        "If you want to display only the content of the response, you can use the following command:"
      ],
      "metadata": {
        "id": "q8g-olaKbwGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "CN65bF2NakzM",
        "outputId": "bf14fe77-b602-4fda-c987-922109b07533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I'm just a language model, so I don't have emotions or feelings like humans do. However, I'm functioning properly and ready to assist you with any questions or tasks you may have! How can I help you today?"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering and Tool Integration\n"
      ],
      "metadata": {
        "id": "GfgYubBVQt1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we dive into the challenge, it's essential to understand the fundamentals of **prompt engineering** and **tool integration**. In this section, we'll explore how to effectively craft prompts to guide the Large Language Model (LLM) in generating precise and relevant outputs.\n",
        "\n",
        "Additionally, we'll learn how to implement external tools that expand the LLM's capabilities, making it more versatile in handling complex tasks. Understanding these concepts will set a strong foundation for successfully completing the challenge ahead."
      ],
      "metadata": {
        "id": "5fd3lbNNcc0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Engineering"
      ],
      "metadata": {
        "id": "4Ya9bvXnOqcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an example of how to adjust the behavior of the LLM by setting a prompt template. In this case, the model is asked to respond in the style of the ancient philosopher Socrates. Feel free to experiment with different prompt styles and templates to observe how the LLM reacts to various scenarios."
      ],
      "metadata": {
        "id": "n7mQ6nDcdqmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt_template = \"\"\"\n",
        "You are Socrates, the ancient philosopher known for your method of questioning to stimulate critical thinking. Respond to the following question using this style.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer (in the style of Socratic dialogue):\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = ChatPromptTemplate.from_template(example_prompt_template)"
      ],
      "metadata": {
        "id": "kAm8OQFcOpu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**LCEL** (LangChain Expression Language)](https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel) is a powerful tool within LangChain that allows you to chain operations in a clean and intuitive way. Using the `|` (pipe) operator, LCEL simplifies the process of connecting components like prompts, language models, and tools.\n",
        "\n",
        "For example, instead of writing multiple lines of code to pass a prompt into a model, you can use LCEL to combine them in a single, elegant expression as below:"
      ],
      "metadata": {
        "id": "1_-0bXrEePyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_v1 = example_prompt | model"
      ],
      "metadata": {
        "id": "yn_x0ELjO7iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model_v1.invoke({\"question\": \"How are you?\"})\n",
        "\n",
        "display(Markdown(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "qYbtra0KPMRZ",
        "outputId": "ef5d6443-a27c-4e96-e67f-808c8b184243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "My dear friend, I must ask, what do you mean by \"how am I?\" Are you inquiring about my physical state? My health? Or perhaps you wish to know the nature of my soul or my inner being?\n\nTell me, how is it that you would define a person's well-being? Is it not through their actions, their thoughts, and their relationships with others? And if so, then I must ask, what is it about myself that you seek to know?\n\nAm I merely a vessel for the gods, or do I possess some inherent quality that makes me who I am? Or perhaps my \"how\" is but a fleeting moment in time, subject to change and impermanence?\n\nLet us explore this question together, my friend. What is it about your inquiry into my state that you hope to uncover?"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tool Integration"
      ],
      "metadata": {
        "id": "HuGValg2Pxpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we’ll explore how to integrate external tools with the LLM to extend its capabilities. Tools enable the model to perform tasks beyond generating text, such as retrieving data or executing functions. This is crucial for making the model more dynamic and interactive.\n",
        "\n",
        "Below is an example of how to create and integrate a tool using the `@tool` decorator. Notice the importance of the **docstring**, as it is used to define what the tool does - so a docstring MUST be provided.. The model reads the docstring to understand how to interact with the tool, so it’s essential to clearly describe the tool’s functionality."
      ],
      "metadata": {
        "id": "lh0uzNisgJ-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"\n",
        "    Get the current weather for a given location.\n",
        "\n",
        "    Args:\n",
        "    location (str): The name of the location to get the weather for.\n",
        "\n",
        "    Returns:\n",
        "    str: A string describing the current weather.\n",
        "    \"\"\"\n",
        "\n",
        "    if location.lower() == \"milan\":\n",
        "        return \"rainy\"\n",
        "    else:\n",
        "        return \"sunny\"\n"
      ],
      "metadata": {
        "id": "ek7OtcSfP3V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `bind_tools` method is used to link external tools to the language model, allowing the model to invoke these tools when necessary. In the example below, the `get_current_weather` tool is bound to the model:"
      ],
      "metadata": {
        "id": "DLhyeulMigvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_v2 = model.bind_tools(tools=[get_current_weather])"
      ],
      "metadata": {
        "id": "kz_mxOBuQVeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How is the weather in Milan?\"\n",
        "messages = [HumanMessage(query)]\n",
        "response_1 = model_v2.invoke(messages)\n",
        "\n",
        "response_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r68DqVpQY35",
        "outputId": "928aca18-537c-434c-b0d1-5dd5c4fe8f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2024-10-22T09:31:30.834084803Z', 'message': {'role': 'assistant', 'content': '', 'tool_calls': [{'function': {'name': 'get_current_weather', 'arguments': {'location': 'Milan'}}}]}, 'done_reason': 'stop', 'done': True, 'total_duration': 500357705, 'load_duration': 57697928, 'prompt_eval_count': 196, 'prompt_eval_duration': 25662000, 'eval_count': 19, 'eval_duration': 375130000}, id='run-55eab94a-76d9-4d48-964a-188fffd9ed49-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'Milan'}, 'id': '07fe3731-ba7d-4498-bdbd-2896f6529fcc', 'type': 'tool_call'}], usage_metadata={'input_tokens': 196, 'output_tokens': 19, 'total_tokens': 215})"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see from the output, the AI correctly identified and called the `get_current_weather` tool with the location set to \"Milan\". However, the tool has not been executed yet, as indicated by the empty content in the response.\n",
        "\n",
        "The below loop manually invokes the tool for each call, and the result (\"rainy\" for Milan) is appended to the conversation.\n"
      ],
      "metadata": {
        "id": "OIFFV0mHjlst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(response_1)\n",
        "\n",
        "for tool_call in response_1.tool_calls:\n",
        "    selected_tool = {\"get_current_weather\": get_current_weather}[tool_call[\"name\"].lower()]\n",
        "    tool_msg = selected_tool.invoke(tool_call)\n",
        "    messages.append(tool_msg)\n",
        "\n",
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRFIOlZOS1Gv",
        "outputId": "a5d64efa-7a1a-4e00-ead7-e25edc26f521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='How is the weather in Milan?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2024-10-22T09:31:30.834084803Z', 'message': {'role': 'assistant', 'content': '', 'tool_calls': [{'function': {'name': 'get_current_weather', 'arguments': {'location': 'Milan'}}}]}, 'done_reason': 'stop', 'done': True, 'total_duration': 500357705, 'load_duration': 57697928, 'prompt_eval_count': 196, 'prompt_eval_duration': 25662000, 'eval_count': 19, 'eval_duration': 375130000}, id='run-55eab94a-76d9-4d48-964a-188fffd9ed49-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'Milan'}, 'id': '07fe3731-ba7d-4498-bdbd-2896f6529fcc', 'type': 'tool_call'}], usage_metadata={'input_tokens': 196, 'output_tokens': 19, 'total_tokens': 215}),\n",
              " ToolMessage(content='rainy', name='get_current_weather', tool_call_id='07fe3731-ba7d-4498-bdbd-2896f6529fcc')]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the tool (`get_current_weather`) is executed and its result (\"rainy\") is added to the `messages`, the updated conversation (including the tool result) is sent back to the LLM:"
      ],
      "metadata": {
        "id": "NtanTIVkkJ3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_2 = model_v2.invoke(messages)\n",
        "\n",
        "display(Markdown(response_2.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "ymZ-UgrjTVGo",
        "outputId": "64f751af-d791-4818-c8ee-88f18c820077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The current weather in Milan is rainy. If you'd like to know more about the forecast or conditions, I can try to find that information for you. Would you like me to check the weather forecast for Milan?"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we manually handled the tool invocation and passed the result back to the LLM. However, this process will be managed dynamically once we create an agent with **LangGraph**. The agent will automatically identify, invoke tools, and integrate their results into the conversation flow without manual intervention, making the workflow more efficient and streamlined.\n",
        "\n",
        "For more examples and different implementations of tools you can check the [documantation](https://python.langchain.com/v0.2/docs/concepts/#tools)"
      ],
      "metadata": {
        "id": "13HingQEkj9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenge"
      ],
      "metadata": {
        "id": "707gSPDqTnfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this challenge, we will provide historical data and metadata obtained from **Yahoo Finance**. Your task is to create an agent capable of analyzing this data to:\n",
        "\n",
        "1. **Generate automatic reports**: The agent should summarize key financial metrics and trends based on the data.\n",
        "2. **Answer user queries**: The agent should respond to specific questions about the data, offering insights and explanations.\n",
        "\n",
        "The goal is to integrate prompt engineering, tool usage, and LangGraph to build an intelligent system that can interact with financial data dynamically and provide meaningful feedback to the user.\n"
      ],
      "metadata": {
        "id": "stifN4RNUANc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Import Data"
      ],
      "metadata": {
        "id": "QrAL-SzDaxiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a dataset, we use **OpenBB** to import financial data from Yahoo Finance. The dataset consists of two parts:\n",
        "- **Historical Price Data**: This includes daily stock prices for a variety of companies.\n",
        "- **Metadata**: Provides additional information about the companies, such as market capitalization, sector, and key financial metrics.\n",
        "\n",
        "To make the process easier, we’ve already extracted the data and uploaded it to GitHub for seamless import into your notebook. You can find the details of the dataset in the repository:\n",
        "\n",
        "[https://github.com/havvanilsuoz/polimi_fintech](https://github.com/havvanilsuoz/polimi_fintech)\n",
        "\n",
        "This allows you to quickly access and work with the data without needing to fetch it directly from Yahoo Finance."
      ],
      "metadata": {
        "id": "dnjcAFIGa0p0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the URLs to each dataset"
      ],
      "metadata": {
        "id": "N7ISIwJVfTSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_historical_1 = 'https://raw.githubusercontent.com/havvanilsuoz/polimi_fintech/refs/heads/main/data/historical_data_1.csv'\n",
        "url_historical_2 = 'https://raw.githubusercontent.com/havvanilsuoz/polimi_fintech/refs/heads/main/data/historical_data_2.csv'\n",
        "url_metadata = 'https://raw.githubusercontent.com/havvanilsuoz/polimi_fintech/refs/heads/main/data/metadata.csv'"
      ],
      "metadata": {
        "id": "aac6xsHuayX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To work with the historical price data, we first load the two separate CSV files (`historical_data_1.csv` and `historical_data_2.csv`) into pandas dataframes. Then, we combine them into a single dataframe."
      ],
      "metadata": {
        "id": "YUbQIxsOfaBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "historical_1 = pd.read_csv(url_historical_1)\n",
        "historical_2 = pd.read_csv(url_historical_2)\n",
        "\n",
        "historical_df = pd.concat([historical_1, historical_2], ignore_index=True)"
      ],
      "metadata": {
        "id": "AMOM22q1f_eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "historical_df"
      ],
      "metadata": {
        "id": "rbe6iKyQjZV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected Output\n",
        "\n",
        "After loading and combining the historical price data, the resulting dataframe (`historical_df`) should contain **416,630 rows** and **10 columns**. The columns in the dataframe are as follows:\n",
        "\n",
        "`date`, `open`, `high`, `low`, `close`, `volume`, `split_ratio`, `dividend`, `symbol`, `capital_gains`\n",
        "\n",
        "These columns represent various stock price metrics such as the opening and closing prices, volume, and other financial details like splits, dividends, and capital gains for each stock symbol.\n"
      ],
      "metadata": {
        "id": "8aODb_y0gSRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To work with the company metadata, we load the `metadata.csv` file into a pandas dataframe and set the `symbol` column as the index. We also drop the unnecessary column named `Unnamed: 0` that was likely created during the data export."
      ],
      "metadata": {
        "id": "Z7dwwGZugpAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_df = pd.read_csv(url_metadata, index_col='symbol').drop(columns=['Unnamed: 0'])"
      ],
      "metadata": {
        "id": "cD74q3uEjfvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_df"
      ],
      "metadata": {
        "id": "-_Asu7h_hJLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After loading the metadata, the resulting dataframe (`metadata_df`) contains **1,664 rows** and **49 columns**. This dataframe provides detailed information about various companies, including financial metrics, industry classifications, and performance indicators.\n",
        "\n",
        "The columns in the `metadata_df` are as follows:\n",
        "\n",
        "`country`, `industry`, `sector`, `longBusinessSummary`, `dividendRate`, `dividendYield`, `payoutRatio`, `beta`, `trailingPE`, `forwardPE`, `volume`, `averageVolume`, `marketCap`, `fiftyTwoWeekLow`, `fiftyTwoWeekHigh`, `currency`, `enterpriseValue`, `profitMargins`, `floatShares`, `sharesOutstanding`, `bookValue`, `priceToBook`, `netIncomeToCommon`, `trailingEps`, `pegRatio`, `lastSplitFactor`, `lastSplitDate`, `enterpriseToRevenue`, `enterpriseToEbitda`, `exchange`, `quoteType`, `underlyingSymbol`, `shortName`, `longName`, `firstTradeDateEpochUtc`, `totalCash`, `totalCashPerShare`, `ebitda`, `totalDebt`, `currentRatio`, `totalRevenue`, `debtToEquity`, `revenuePerShare`, `returnOnAssets`, `returnOnEquity`, `freeCashflow`, `operatingCashflow`, `operatingMargins`, `financialCurrency`\n",
        "\n",
        "These columns provide comprehensive data points for each company, such as:\n",
        "- **Industry and Sector Information**: Industry and sector classifications.\n",
        "- **Financial Ratios**: Ratios like price-to-earnings (P/E), return on assets (ROA), and return on equity (ROE).\n",
        "- **Stock Performance Metrics**: Market cap, 52-week high/low, and dividend information.\n",
        "- **Company Overview**: Basic details like company names and long business summaries.\n",
        "\n",
        "This data is essential for performing detailed financial analysis and comparisons."
      ],
      "metadata": {
        "id": "ei4WC1GUg-hr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools"
      ],
      "metadata": {
        "id": "CFXadmWqZ9e9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As part of the challenge, it is expected that you create and implement the following tools:\n",
        "\n",
        "- **`generate_report`**: Generates a summary report for the top 5 companies based on financial performance and key metrics.\n",
        "- **`plot_historical_price_data`**: Plots historical price data for a given list of stock symbols, with the option to specify a date range for more focused analysis.\n",
        "- **`search`**: A tool to search for detailed explanations and news about specific tickers or companies. It is suggested to use **DuckDuckGoSearchRun** for this functionality.\n",
        "- **`ticker_description`**: Provides descriptive statistics for a given symbol from the historical dataset, giving an overview of performance.\n",
        "\n",
        "These tools are suggested starting points, but feel free to get creative and add additional tools or features that can enhance the agent's functionality and its interaction with financial data."
      ],
      "metadata": {
        "id": "KREBBtvMvx6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool 1: Report"
      ],
      "metadata": {
        "id": "Vi1rLOek0EkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will implement a tool called `generate_report` that provides a comprehensive financial analysis of a set of companies. Here are the components you need to include in your report:\n",
        "\n",
        "1. **Comparison of Key Financial Metrics**: Generate a table comparing metrics like profit margins, return on equity (ROE), return on assets (ROA), and price-to-earnings ratios (trailing and forward) for the top 5 companies.\n",
        "2. **Company Strengths and Weaknesses**: Analyze each company's financial strengths and weaknesses based on their performance relative to their peers.\n",
        "3. **Company Details**: Include details like the most recent stock prices, six-month returns, market cap, and 52-week high/low prices.\n",
        "\n",
        "Instructions:\n",
        "- Use the provided dataset to prepare the necessary financial metrics and company data.\n",
        "- Ensure that your tool outputs a report in markdown format, making it easy to read and interpret.\n",
        "- Incorporate visual elements such as pie charts and historical price line plots to make your report more engaging and informative.\n",
        "- Be creative and feel free to extend the tool with additional insights, data points, or custom visualizations.\n",
        "\n",
        "The goal is to create a detailed and interactive tool that delivers a comprehensive analysis, leveraging both data and visual representations."
      ],
      "metadata": {
        "id": "sF_fEFKy1nAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def generate_report() -> str:\n",
        "    \"\"\"\n",
        "    Generates a comprehensive financial analysis report for a set of companies.\n",
        "\n",
        "    The report includes:\n",
        "    1. A comparison of key financial metrics such as profit margins, return on equity,\n",
        "       return on assets, price-to-earnings ratios (trailing and forward), revenue growth,\n",
        "       and earnings growth.\n",
        "    2. A detailed analysis of each company's strengths and weaknesses based on these metrics\n",
        "       compared to their peers.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        str: A markdown-formatted financial analysis report containing:\n",
        "            - A comparison table of key metrics.\n",
        "            - A comparison table of company details.\n",
        "            - A breakdown of each company's financial strengths and weaknesses.\n",
        "    \"\"\"\n",
        "    # TO COMPLETE: Implement the logic to generate the financial report.\n",
        "    # Use the provided dataset to calculate and compare the key metrics.\n",
        "    # Analyze each company's performance and output the results as a markdown report.\n",
        "\n",
        "    # Example steps:\n",
        "    # 1. Prepare the data.\n",
        "    # 2. Generate tables comparing key financial metrics.\n",
        "    # 3. Analyze company strengths and weaknesses.\n",
        "    # 4. Output the results in markdown format.\n",
        "\n"
      ],
      "metadata": {
        "id": "qn14GJRGZ8lG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example Output"
      ],
      "metadata": {
        "id": "4bQ9EqJR3SCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PUT IMAGE OF SOLUTION RESPOND"
      ],
      "metadata": {
        "id": "GS3QW2O13uAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool 2: Plot Historical Data"
      ],
      "metadata": {
        "id": "KSTsu3nN51uO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tool is designed to plot historical price data for a list of stock symbols within an optional date range. The function should:\n",
        "\n",
        "1. Take a list of stock symbols (`symbol_list`) as input.\n",
        "2. Optionally, take a start date and an end date (`start_date`, `end_date`) to filter the historical data.\n",
        "3. Plot the historical price data for the provided symbols over the specified date range.\n",
        "4. Save the plot as an image and return the file path to the saved plot.\n",
        "\n",
        "Arguments:\n",
        "- `symbol_list (List[str])`: The list of stock symbols to plot.\n",
        "- `start_date (str, optional)`: The start date for the plot in the format `'YYYY-MM-DD'` (optional).\n",
        "- `end_date (str, optional)`: The end date for the plot in the format `'YYYY-MM-DD'` (optional).\n",
        "\n",
        "Returns:\n",
        "- `str`: The file path to the saved plot image (`historical_plot.png`).\n",
        "\n",
        "Instructions:\n",
        "Your task is to complete this function by implementing the logic for filtering the data, creating the plot, and saving the image. Use `matplotlib` for plotting and ensure the date range is respected if provided."
      ],
      "metadata": {
        "id": "Xz3kFt5vHUnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def plot_historical_price_data(symbol_list: List[str], start_date: str = None, end_date: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Plots historical price data for a list of stock symbols within an optional date range.\n",
        "\n",
        "    Args:\n",
        "        symbol_list (List[str]): List of stock symbols to plot.\n",
        "        start_date (str, optional): Start date ('YYYY-MM-DD'). Defaults to None (all available data).\n",
        "        end_date (str, optional): End date ('YYYY-MM-DD'). Defaults to None (all available data).\n",
        "\n",
        "    Returns:\n",
        "        str: File path to the saved plot image ('historical_plot.png').\n",
        "    \"\"\"\n",
        "\n",
        "    # TO COMPLETE: Implement logic to filter historical data by date range and create a plot.\n",
        "    # Steps:\n",
        "    # 1. Convert date strings to datetime format.\n",
        "    # 2. Filter the historical data for the specified symbols and date range.\n",
        "    # 3. Create a plot using matplotlib.\n",
        "    # 4. Save the plot as 'historical_plot.png'.\n",
        "    # 5. Return the file path to the saved plot image.\n",
        "\n",
        "    TO COMPLETE"
      ],
      "metadata": {
        "id": "q5mRpY1-5-N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool 3: Search"
      ],
      "metadata": {
        "id": "WrLRR9YeUVSY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `search` tool uses **DuckDuckGoSearchRun**, which is a pre-built tool that leverages the DuckDuckGo search engine to retrieve relevant information from the web. This tool is useful for fetching external data, detailed explanations, or news related to specific tickers or companies. It allows the model to answer user queries by performing live searches and returning web results.\n",
        "\n",
        "What is DuckDuckGoSearchRun?\n",
        "**DuckDuckGoSearchRun** is a search utility that interacts with the DuckDuckGo search engine, which emphasizes privacy and provides quick, accurate search results. It’s helpful when you need information that isn’t available in the dataset or when you need up-to-date information from the web.\n",
        "\n",
        "Instructions:\n",
        "Your task is to write a clear and proper **description** for this tool, so that the LLM understands when to use it. The description should explain the specific scenarios where the tool would be relevant, such as when the model needs to provide detailed explanations, search for news, or retrieve external information about companies or stock tickers.\n"
      ],
      "metadata": {
        "id": "De_CDIDUH_No"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search = DuckDuckGoSearchRun(\n",
        "    name=\"search\",\n",
        "    description=\"TO COMPLETE\"\n",
        ")"
      ],
      "metadata": {
        "id": "zp7J4Ih4UaS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool 4: Ticker Description"
      ],
      "metadata": {
        "id": "iEBYiXNNtFd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The `ticker_description` tool is designed to return the descriptive statistics for a given stock ticker symbol. This function filters the historical data by the specified stock symbol and provides key statistical summaries such as opening price, high, low, close, and volume.\n",
        "\n",
        "Arguments:\n",
        "- `symbol (str)`: The stock ticker symbol to filter.\n",
        "\n",
        "Returns:\n",
        "- `str`: A markdown-formatted output of the descriptive statistics or a message indicating no data was found for the symbol.\n",
        "\n",
        "Instructions:\n",
        "1. The tool takes a stock ticker symbol (`symbol`) as an argument.\n",
        "2. It filters the historical data (`historical_df`) for the given symbol.\n",
        "3. It returns the descriptive statistics for that symbol's historical data in markdown format. The statistics include data like open, high, low, close, and volume."
      ],
      "metadata": {
        "id": "6huqUbtyITzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def ticker_description(symbol: str) -> str:\n",
        "    \"\"\"\n",
        "    Returns the descriptive statistics for the given symbol from the historical_df DataFrame.\n",
        "\n",
        "    Args:\n",
        "        symbol (str): The stock ticker symbol to filter.\n",
        "\n",
        "    Returns:\n",
        "        str: The .describe() output for the symbol's historical data in markdown format.\n",
        "    \"\"\"\n",
        "    # TO COMPLETE: Implement logic to filter historical data and generate descriptive statistics.\n",
        "    # Steps:\n",
        "    # 1. Filter the DataFrame for the given symbol.\n",
        "    # 2. Check if data exists for the symbol, return an appropriate message if not.\n",
        "    # 3. Generate descriptive statistics using the .describe() method.\n",
        "    # 4. Format the output in markdown.\n",
        "\n",
        "    TO COMPLETE"
      ],
      "metadata": {
        "id": "kNwQqSu3tO6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool 5 and More (Optional)"
      ],
      "metadata": {
        "id": "oH5fGMahI_JD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to the core tools, you are encouraged to create any additional tools that you think would be useful for further analysis or insights. These tools could perform deeper data analysis, generate visualizations, or answer specific queries based on the data. This part is completely optional, but feel free to get creative and explore new ways to analyze or visualize the financial data.\n",
        "\n",
        "Some ideas for additional tools could include:\n",
        "- Calculating moving averages or other technical indicators.\n",
        "- Performing sentiment analysis on company news.\n",
        "- Creating custom visualizations or dashboards.\n",
        "- Comparing companies based on specific performance metrics.\n",
        "\n",
        "Let your imagination guide you—there are no limits to the kinds of tools you can create to enhance the project."
      ],
      "metadata": {
        "id": "2VczEpMAIeCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompts"
      ],
      "metadata": {
        "id": "UW0chWCSBYeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we will work with two different types of prompts to guide the LLM’s behavior and outputs:\n",
        "\n",
        "1. **Chat Prompt**: This will serve as the main prompt that the LLM uses to answer general questions from users. It will handle conversational queries and respond dynamically based on the tools and data available.\n",
        "   \n",
        "2. **Report Prompt**: This is a custom-designed prompt specifically tailored to generate a structured output report. The goal is to create a prompt that ensures the LLM produces a well-organized and comprehensive financial report, including analysis, comparisons, and summaries.\n",
        "\n",
        "These prompts allow us to control how the LLM interacts with both user queries and structured report generation."
      ],
      "metadata": {
        "id": "MpH2HaJCJi5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt 1: Chat"
      ],
      "metadata": {
        "id": "mOrwVHxkM8U5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When writing the main **Chat Prompt**, it’s important to ensure that the LLM behaves reliably and uses the tools effectively. The Chat Prompt should instruct the model to avoid hallucination or making up data, and it should clearly indicate that the model should rely on the tools to fetch accurate information.\n",
        "\n",
        "Tips for Writing the Chat Prompt:\n",
        "- **No Hallucination**: Explicitly tell the model not to guess, fabricate, or provide information without reliable data sources.\n",
        "- **Stick to Tools**: Make it clear that the LLM should use the tools you’ve integrated to answer questions rather than attempting to import or create data on its own.\n",
        "- **Professionalism and Clarity**: Ensure the prompt maintains a professional tone and instructs the LLM to be clear and precise in all responses.\n",
        "\n",
        "A well-crafted prompt will ensure the LLM remains accurate, professional, and effective in answering questions based on the tools.\n"
      ],
      "metadata": {
        "id": "5cVDU0oEKF1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt_template = \"\"\"\n",
        "\n",
        "COMPLETE THE PROMPT\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HWC8Snx_M3Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt 2: Report"
      ],
      "metadata": {
        "id": "Nh9ch-6PBaTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When designing the **Report Prompt**, you want to ensure the LLM generates a structured, comprehensive financial analysis report based strictly on the data provided by the tools. The prompt should clearly instruct the model to avoid fabricating any data and follow the predefined structure.\n",
        "\n",
        "#### Tips for Writing the Report Prompt:\n",
        "- **No Hallucination**: Instruct the model to avoid fabricating any information and only use the results from the tools.\n",
        "- **Structured Output**: Define a specific structure for the report so that the LLM produces organized, easy-to-read outputs. You can specify section headers, tables, and paragraphs for each part of the analysis.\n",
        "- **Clear and Consistent Layout**: Make sure to guide the model on how to present the data with consistency in formatting and detail.\n",
        "\n",
        "For example, you can create a prompt that outlines the report structure with headers like:\n",
        "- **Dataset Overview**: Explaining the number of companies and sector breakdown.\n",
        "- **Company Overview**: Describing the companies included in the analysis.\n",
        "- **Comparison Table**: Comparing key financial metrics such as Profit Margins, ROE, ROA, and P/E ratios.\n",
        "- **Company Analysis**: Summarizing each company's strengths and weaknesses.\n",
        "- **Market Capitalization and Price History**: Detailing metrics like market cap, current price, and returns.\n",
        "- **Key Insights**: Highlighting the major findings and trends.\n",
        "\n",
        "By explicitly defining these sections, you ensure the LLM produces a structured report that meets the requirements."
      ],
      "metadata": {
        "id": "UCwV6_GLKkZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report_prompt_template = \"\"\"\n",
        "\n",
        "TO COMPLETE\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hw7rl337-zBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangGraph"
      ],
      "metadata": {
        "id": "aVzsipwkIriq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**LangGraph** is a framework that simplifies the integration of tools and models by organizing them into a graph-based structure. This allows for more dynamic and responsive AI systems. In LangGraph, the system is designed using **nodes** and **edges**, which define the relationships and flow of operations.\n",
        "\n",
        "Nodes and Edges in LangGraph:\n",
        "- **Nodes**: Each node represents a specific component or operation. For example, a node could be a tool (such as `generate_report` or `search`) or a prompt template that the model uses to generate responses. Nodes are the building blocks of LangGraph, and each one performs a distinct function.\n",
        "  \n",
        "- **Edges**: Edges define the relationships between nodes. They determine how data or results from one node are passed to another. For instance, an edge could connect a prompt node to a tool node, indicating that the result from the prompt should trigger the tool to execute. This creates a logical flow where operations depend on the output of previous nodes."
      ],
      "metadata": {
        "id": "LiwoNeNSLmiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `AgentState`"
      ],
      "metadata": {
        "id": "B225jHdlMLLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `AgentState` class is a custom type definition using Python’s **`TypedDict`** and annotations to define the structure of an agent’s state in a conversation.\n",
        "\n",
        "- **`TypedDict`**: A Python feature that allows you to define dictionaries with a specific structure, including the types of values that each key should hold. In this case, `AgentState` defines a dictionary where the key is `messages` and the value is a list of messages.\n",
        "  \n",
        "- **`messages: Annotated[list[AnyMessage], add_messages]`**:\n",
        "  - **`list[AnyMessage]`**: This specifies that the `messages` key should hold a list of `AnyMessage` objects, which represent the different types of messages the agent handles (e.g., user queries, tool responses, etc.).\n",
        "  - **`Annotated`**: An annotation that allows for additional metadata to be associated with the `messages`. In this case, the `add_messages` function is added as metadata, possibly to modify or validate the `messages` list dynamically.\n",
        "  \n",
        "Purpose:\n",
        "This structure is used to track the agent's state, particularly the conversation history (`messages`). The `add_messages` annotation likely provides additional functionality, such as updating or appending new messages during the interaction.\n",
        "\n",
        "Overall, the `AgentState` class provides a typed and annotated way to maintain the conversation flow within the agent."
      ],
      "metadata": {
        "id": "sJE4WLvQL3Ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add_messages]"
      ],
      "metadata": {
        "id": "ECo3lF34Ig27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binding Tools to the LLM\n",
        "\n",
        "Once you've created the tools (e.g., `generate_report`, `plot_historical_price_data`, `search`, etc.), you should combine them into a list and bind them to the LLM. This allows the LLM to call the appropriate tool when needed."
      ],
      "metadata": {
        "id": "G7lhe_SnMPzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [generate_report, plot_historical_price_data, search, ticker_description]\n",
        "model_with_tool = model.bind_tools(tools)"
      ],
      "metadata": {
        "id": "G2HA-zKhzbVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nodes"
      ],
      "metadata": {
        "id": "Y9UHHHFbMUQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we define how to build and manage **Nodes** that interact with tools and handle potential errors. These nodes are responsible for managing the flow of information between the tools and the LLM during the conversation."
      ],
      "metadata": {
        "id": "mZGB2HGHMrKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`chatbot`:\n",
        "The `chatbot` function processes the conversation state and decides which prompt (either `report` or `chat`) to use based on the last message received. It then prepares the message with the appropriate prompt and invokes the model with the list of messages.\n",
        "\n",
        "- **Messages Handling**: If the last message is from a tool (like `generate_report`), the function uses the `report_prompt_template`. Otherwise, it defaults to the `chat_prompt_template`.\n",
        "- **Prompt Insertion**: A `SystemMessage` is created from the chosen prompt and added to the list of messages.\n",
        "- **Model Invocation**: The model is invoked with the updated list of messages and tools."
      ],
      "metadata": {
        "id": "U7X13mC9Ms16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot(state: AgentState):\n",
        "  messages = state['messages']\n",
        "  if messages and isinstance(messages[-1], ToolMessage):\n",
        "    if messages[-1].name == \"generate_report\":\n",
        "      prompt_template = report_prompt_template\n",
        "    else:\n",
        "      prompt_template = chat_prompt_template\n",
        "\n",
        "  else:\n",
        "    prompt_template = chat_prompt_template\n",
        "\n",
        "  prompt = ChatPromptTemplate.from_messages(prompt_template)\n",
        "\n",
        "  # Create the HumanMessage from the chosen template\n",
        "  prompt_message = SystemMessage(content=prompt_template)\n",
        "\n",
        "  # Add the prompt to the messages list\n",
        "  messages_with_prompt = [prompt_message] + messages\n",
        "\n",
        "  # Invoke the model with the current messages and prompt\n",
        "  response = model_with_tool.invoke(messages_with_prompt)\n",
        "\n",
        "  #response = model_with_tool.invoke(messages)\n",
        "  return {\"messages\": [response]}"
      ],
      "metadata": {
        "id": "MAeMgVT9IuEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`create_tool_node_with_fallback`:\n",
        "This function creates a **ToolNode** that connects the list of tools to the model. The node also includes a fallback mechanism, ensuring that if an error occurs when invoking a tool, it can handle it gracefully.\n",
        "\n",
        "- **ToolNode**: This node is responsible for managing interactions with the tools.\n",
        "- **Fallbacks**: The node includes fallbacks using `RunnableLambda` to handle potential tool errors. If an exception occurs, the fallback handles the error and prevents the system from crashing.\n",
        "\n",
        "`handle_tool_error`:\n",
        "This function defines how to manage errors when invoking tools. It retrieves the error message and the last tool call, then returns an error message indicating that something went wrong and asking the user to correct it.\n",
        "\n",
        "- **Error Handling**: The function looks for any errors in the conversation state and returns a `ToolMessage` indicating the issue.\n",
        "- **Tool Call Reference**: The error message refers to the specific tool call that failed, making it easier for users to identify and fix the issue."
      ],
      "metadata": {
        "id": "X8itpzuYM5uX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tool_node_with_fallback(tools: list) -> dict:\n",
        "    return ToolNode(tools).with_fallbacks(\n",
        "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
        "    )\n",
        "\n",
        "\n",
        "def handle_tool_error(state: AgentState) -> dict:\n",
        "    error = state.get(\"error\")\n",
        "    tool_calls = state[\"messages\"][-1].tool_calls\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            ToolMessage(\n",
        "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
        "                tool_call_id=tc[\"id\"],\n",
        "            )\n",
        "            for tc in tool_calls\n",
        "        ]\n",
        "    }\n"
      ],
      "metadata": {
        "id": "18C3XUZ7KDqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Condition for Edges"
      ],
      "metadata": {
        "id": "yDWhOIPnM-kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In LangGraph, **Edges** represent the flow between nodes, dictating how and when data moves from one node to another. Conditions on edges determine the next step in the process based on the current state of the conversation.\n",
        "\n",
        "`should_continue`:\n",
        "The `should_continue` function acts as a condition that decides the next step in the conversation flow. Based on the state of the conversation (i.e., the last message), this function determines whether the flow should proceed to a tool node or stop the conversation.\n",
        "\n",
        "- **Messages Handling**: The function checks the list of messages in the conversation state.\n",
        "- **Tool Call Check**: If the last message contains tool calls (i.e., a tool was invoked), the function returns `\"tool_node\"`, indicating that the flow should proceed to the tool node to handle the tool execution.\n",
        "- **End Condition**: If there are no tool calls in the last message, the function returns `END`, meaning the conversation flow should stop.\n",
        "\n",
        "This conditional logic ensures that the conversation either continues to the tool node (if a tool is required) or concludes the process if no further actions are needed."
      ],
      "metadata": {
        "id": "3UiaMHm2NcU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def should_continue(state: AgentState):\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    if last_message.tool_calls:\n",
        "        return \"tool_node\"\n",
        "    return END"
      ],
      "metadata": {
        "id": "EGCrMuGiPlpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define LangGraph"
      ],
      "metadata": {
        "id": "5vNCWhCsNFlc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create our LangGraph using the components we have developed so far. We will combine the chatbot, tool nodes, and conditional logic into a structured workflow.\n",
        "\n",
        "- **Define the Graph**: We start by defining a state graph using `StateGraph(AgentState)` to manage the conversation's state.\n",
        "- **Add Nodes**:\n",
        "  - `chatbot`: This node processes user queries and selects the appropriate prompt (chat or report).\n",
        "  - `tool_node`: This node manages the execution of tools, such as `generate_report` or `search`, with fallback handling for any errors.\n",
        "- **Add Edges**:\n",
        "  - We connect the `chatbot` node to the start (`START`), and use the `should_continue` function to check if the flow should move to the `tool_node` or end.\n",
        "  - After the tool is executed, the conversation flow returns to the `chatbot` node for further processing.\n",
        "- **Compile the Workflow**: Finally, we compile the graph into an operational app (`app`), enabling dynamic conversation management where the LLM can handle queries, invoke tools, and manage errors."
      ],
      "metadata": {
        "id": "UDjy3EC6OAfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a new graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node('chatbot', chatbot)\n",
        "workflow.add_node('tool_node', create_tool_node_with_fallback(tools))\n",
        "\n",
        "workflow.add_edge(START, 'chatbot')\n",
        "workflow.add_conditional_edges('chatbot', should_continue)\n",
        "\n",
        "workflow.add_edge(\"tool_node\", \"chatbot\")\n",
        "\n",
        "app = workflow.compile()\n"
      ],
      "metadata": {
        "id": "_9c_84GoKHPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's visualize the graph we have created to see how the conversation and tool nodes are connected. The following code attempts to display the graph in an image format"
      ],
      "metadata": {
        "id": "SWH0eF1SOLT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    display(Image(app.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "xKoH_t53QAgp",
        "outputId": "f90f96e8-ac88-4cbe-e9ee-0f9a7dfae939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try Your Agent!"
      ],
      "metadata": {
        "id": "9a35w4sG8srw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that your agent is fully set up, it should be able to answer a wide range of questions by leveraging the tools you’ve created. Here are some example queries to try:\n",
        "\n",
        "- **\"Can you generate the report?\"**: This should trigger the agent to create a financial analysis report based on the top 5 companies.\n",
        "- **\"Can you plot the historical price data of NVO?\"**: The agent should be able to plot the historical price data for the specified ticker.\n",
        "- **\"Can you plot the historical price data of NVO and ASML from February 2023 to July 2023?\"**: The agent will use the specified date range and symbols to create a historical price chart.\n",
        "- **\"What is the ticker SAP? Can you explain?\"**: The agent should retrieve detailed information about the ticker SAP.\n",
        "- **\"Can you provide me the key statistics of the historical data of the ticker NVO?\"**: The agent should return the descriptive statistics for the given ticker symbol based on the historical data.\n",
        "\n",
        "Feel free to ask these questions and more to test the agent’s capabilities!"
      ],
      "metadata": {
        "id": "34TzvMWWPutt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = app.invoke({\"messages\": (\"user\", \"Can you generate the report?\")})\n",
        "\n",
        "display(Markdown(response['messages'][-1].content))"
      ],
      "metadata": {
        "id": "aevLuqxXQ76X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Take It Further: Improve Your Data Analysis Agent!"
      ],
      "metadata": {
        "id": "31tnbXtIQlu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you've created a functional data analysis agent, it's time to get creative! You can improve and expand on this project with your team by:\n",
        "\n",
        "- Adding more tools for in-depth data analysis.\n",
        "- Enhancing visualizations with advanced plots and charts.\n",
        "- Implementing additional features, like predictive models or real-time data fetching.\n",
        "- Making the agent more complex with new workflows or nodes.\n",
        "- Improving the user interface for a better experience.\n",
        "\n",
        "We encourage you to experiment, collaborate with your team, and make this agent as robust and innovative as possible.\n",
        "\n",
        "### Share Your Work with Us\n",
        "\n",
        "Once you’ve made your improvements, please submit the notebook by filling out the following details:\n",
        "\n",
        "- **Team Member Names**:\n",
        "- **Email Addresses**:\n",
        "\n",
        "Please send the completed notebook to **havvanilsu.oz@iasonltd.com**. We’re excited to see your creativity and ideas! You may be contacted for potential opportunities based on your work.\n"
      ],
      "metadata": {
        "id": "xfYCTg5UQsbY"
      }
    }
  ]
}